{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a76cced7-a209-46ad-ab36-a98899c6d1e4",
   "metadata": {},
   "source": [
    "### Let's start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17f7603e-86a9-4beb-a612-820ba78c708e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ff5a1cc-3028-4483-a1fe-e45519f27346",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_json('C:/Codes/Siena_AI/Synthetic_Data/Training_Dataset.json')\n",
    "val_df = pd.read_json('C:/Codes/Siena_AI/Synthetic_Data/Validation_Dataset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29c6d84e-3c07-438f-925a-19c8732c8938",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment\")\n",
    "\n",
    "# Function to tokenize and encode the data\n",
    "def preprocess_data(data):\n",
    "    return tokenizer(\n",
    "        data['Sentence'].tolist(),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=384,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "# Preprocess both train and validation data\n",
    "train_encodings = preprocess_data(train_df)\n",
    "val_encodings = preprocess_data(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eabe47d8-db9e-4443-a4df-17e15f43d1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Define the number of unique labels\n",
    "num_labels = len(train_df['Sentiment'].unique())\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment\",\n",
    "    num_labels=num_labels,\n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72d0375d-14e2-427d-9b62-0039456cdb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#As we see above, this model was initially trained for binary Classification,\n",
    "# we will have to re-write the configuration\n",
    "label_mappings = {\n",
    "    0: \"Strong Negative\",\n",
    "    1: \"Mild Negative\",\n",
    "    2: \"Neutral\",\n",
    "    3: \"Mild Positive\",\n",
    "    4: \"Strong Positive\"\n",
    "}\n",
    "\n",
    "# Update the model's configuration to use custom id2label and label2id mappings\n",
    "model.config.id2label = label_mappings\n",
    "model.config.label2id = {v: k for k, v in label_mappings.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abe3e169-42d9-4596-a7d1-3bc34d35d0ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment\",\n",
       "  \"architectures\": [\n",
       "    \"BertForSequenceClassification\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"Strong Negative\",\n",
       "    \"1\": \"Mild Negative\",\n",
       "    \"2\": \"Neutral\",\n",
       "    \"3\": \"Mild Positive\",\n",
       "    \"4\": \"Strong Positive\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"Mild Negative\": 1,\n",
       "    \"Mild Positive\": 3,\n",
       "    \"Neutral\": 2,\n",
       "    \"Strong Negative\": 0,\n",
       "    \"Strong Positive\": 4\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.46.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 21128\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's take a look at the model configuration\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35f5bf57-6f62-4e33-9003-4bb5639f3baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 23:28, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.658947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.637957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.625891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.622058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=1.6486251831054688, metrics={'train_runtime': 1413.8335, 'train_samples_per_second': 3.395, 'train_steps_per_second': 0.212, 'total_flos': 241739793388800.0, 'train_loss': 1.6486251831054688, 'epoch': 4.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "# Convert labels to numeric format\n",
    "train_labels = torch.tensor(train_df['Sentiment'].factorize()[0])\n",
    "val_labels = torch.tensor(val_df['Sentiment'].factorize()[0])\n",
    "\n",
    "# Create a Dataset class to work with Trainer\n",
    "class SentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SentimentDataset(train_encodings, train_labels)\n",
    "val_dataset = SentimentDataset(val_encodings, val_labels)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-7,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ae778e5-c909-4e0c-a3bd-3593d877b6c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 00:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.6220580339431763,\n",
       " 'eval_runtime': 23.0043,\n",
       " 'eval_samples_per_second': 13.041,\n",
       " 'eval_steps_per_second': 0.826,\n",
       " 'epoch': 4.0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model and tokenizer\n",
    "model.save_pretrained(\"./fine_tuned_sentiment_model_rt1\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_sentiment_model_rt1\")\n",
    "\n",
    "# Evaluate the model on the validation dataset\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca5b6649-4bd4-4ece-acfe-20166066e97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on the validation set\n",
    "predictions = trainer.predict(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1938bb77-a4c1-4a54-aa4e-fff8f5b4fdb9",
   "metadata": {},
   "source": [
    "### Test Some Sample Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a2472d2-f69c-4358-b9ca-092feb22aa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"fine_tuned_sentiment_model_rt1\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"fine_tuned_sentiment_model_rt1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbad140b-a651-4f7e-95dc-4d220859beca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"text-classification\", model=\"fine_tuned_sentiment_model_rt1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8f6eb9c-8e26-4be3-b9ea-4c81d9474c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = [\"The food was absolutely horrible. The flavors were completely unbalanced and the presentation was terrible. I wouldn\\u2019t recommend this place to my worst enemy.\",\n",
    "             \"While this restaurant had some good dishes, some of my friends ordered the same thing, which resulted in an overcrowded table. The dining area was quite small, which made us feel rushed.\",\n",
    "            \"This place has been on my bucket list for so long! The food is always a highlight, and I love trying new dishes. I highly recommend it to anyone looking for a great dining experience.\",\n",
    "             \"The food was okay, nothing special. It was neither bad nor great, just an average meal.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30bd45ce-d1bb-44f9-b434-595722886237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: The food was absolutely horrible. The flavors were completely unbalanced and the presentation was terrible. I wouldn’t recommend this place to my worst enemy.\n",
      "Predicted Label: Mild Negative,Score: 0.2509\n",
      "\n",
      "Input: While this restaurant had some good dishes, some of my friends ordered the same thing, which resulted in an overcrowded table. The dining area was quite small, which made us feel rushed.\n",
      "Predicted Label: Mild Negative,Score: 0.2828\n",
      "\n",
      "Input: This place has been on my bucket list for so long! The food is always a highlight, and I love trying new dishes. I highly recommend it to anyone looking for a great dining experience.\n",
      "Predicted Label: Strong Negative,Score: 0.2551\n",
      "\n",
      "Input: The food was okay, nothing special. It was neither bad nor great, just an average meal.\n",
      "Predicted Label: Mild Negative,Score: 0.2697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text in test_texts:\n",
    "    result = classifier(text)\n",
    "    print(f\"Input: {text}\")\n",
    "    print(f\"Predicted Label: {result[0]['label']},Score: {result[0]['score']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626f16b5-91f0-4e41-9060-cf0c87fbd90b",
   "metadata": {},
   "source": [
    "### Micro, Macro Precision and Recall Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "553df037-38e3-408d-a44b-8875d516f786",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "828fadd3-5098-4278-84f6-a06b2a072ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get predictions from the model\n",
    "def get_predictions(model, dataset):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in tqdm(dataset):\n",
    "        inputs = {key: batch[key].unsqueeze(0).to(model.device) for key in batch if key != 'labels'}\n",
    "        labels = batch['labels'].to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        # Convert labels and predictions to list format\n",
    "        all_labels.append(labels.item())  # For single labels\n",
    "        all_preds.append(predictions.item())  # For single predictions\n",
    "    \n",
    "    return all_labels, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9719981-6aa4-48c8-b27e-38216267ab9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 300/300 [00:29<00:00, 10.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': {'precision': 0.0851063829787234, 'recall': 0.06666666666666667, 'f1-score': 0.07476635514018691, 'support': 60.0}, '1': {'precision': 0.25766871165644173, 'recall': 0.7, 'f1-score': 0.37668161434977576, 'support': 60.0}, '2': {'precision': 0.25, 'recall': 0.08333333333333333, 'f1-score': 0.125, 'support': 60.0}, '3': {'precision': 0.26785714285714285, 'recall': 0.25, 'f1-score': 0.25862068965517243, 'support': 60.0}, '4': {'precision': 0.07142857142857142, 'recall': 0.016666666666666666, 'f1-score': 0.02702702702702703, 'support': 60.0}, 'accuracy': 0.22333333333333333, 'macro avg': {'precision': 0.18641216178417586, 'recall': 0.22333333333333333, 'f1-score': 0.17241913723443242, 'support': 300.0}, 'weighted avg': {'precision': 0.18641216178417586, 'recall': 0.22333333333333333, 'f1-score': 0.17241913723443245, 'support': 300.0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get predictions and labels from the validation set\n",
    "true_labels, predicted_labels = get_predictions(model, val_dataset)\n",
    "\n",
    "# Compute classification metrics: precision, recall, f1, and support\n",
    "report = classification_report(true_labels, predicted_labels, labels=[0,1,2,3,4], output_dict=True)\n",
    "\n",
    "# Print out the classification report\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e9e0bf1-8f05-4570-987c-2d4ba61c9b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro Precision: 0.18641216178417586\n",
      "Macro Recall: 0.22333333333333333\n",
      "Micro Precision: 0.18641216178417586\n",
      "Micro Recall: 0.22333333333333333\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Extract precision, recall, and support for each class\n",
    "precision_per_class = [report[str(label)]['precision'] for label in range(len(report) - 3)]\n",
    "recall_per_class = [report[str(label)]['recall'] for label in range(len(report) - 3)]\n",
    "support_per_class = [report[str(label)]['support'] for label in range(len(report) - 3)]\n",
    "\n",
    "# Calculate macro-averaged metrics (which are directly available in `report`)\n",
    "macro_precision = report['macro avg']['precision']\n",
    "macro_recall = report['macro avg']['recall']\n",
    "\n",
    "# Calculate micro-averaged metrics\n",
    "# Weighted by the number of samples per class (support)\n",
    "micro_precision = np.sum([p * s for p, s in zip(precision_per_class, support_per_class)]) / np.sum(support_per_class)\n",
    "micro_recall = np.sum([r * s for r, s in zip(recall_per_class, support_per_class)]) / np.sum(support_per_class)\n",
    "\n",
    "# Output calculated values\n",
    "print(\"Macro Precision:\", macro_precision)\n",
    "print(\"Macro Recall:\", macro_recall)\n",
    "print(\"Micro Precision:\", micro_precision)\n",
    "print(\"Micro Recall:\", micro_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1de4792-e840-49fd-87db-a59e1803b790",
   "metadata": {},
   "source": [
    "## Re-Training the model again, differently..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfbeb9a-ee38-470f-919c-386c2d94ab4b",
   "metadata": {},
   "source": [
    "### Let's start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "007a74a7-6ad6-4102-b53e-accfe106747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92887213-3a8d-47d8-9a1d-4064a2d74a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment\")\n",
    "\n",
    "# Function to tokenize and encode the data\n",
    "def preprocess_data(data):\n",
    "    return tokenizer(\n",
    "        data['Sentence'].tolist(),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "# Preprocess both train and validation data\n",
    "train_encodings = preprocess_data(train_df)\n",
    "val_encodings = preprocess_data(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "928d8694-60ac-4708-b67a-d53bee1b9a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Define the number of unique labels\n",
    "num_labels = len(train_df['Sentiment'].unique())\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment\",\n",
    "    num_labels=num_labels,\n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6f71bdc-a0b1-410e-88ab-b586fa4240c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mappings = {\n",
    "    0: \"Strong Negative\",\n",
    "    1: \"Mild Negative\",\n",
    "    2: \"Neutral\",\n",
    "    3: \"Mild Positive\",\n",
    "    4: \"Strong Positive\"\n",
    "}\n",
    "\n",
    "# Update the model's configuration to use custom id2label and label2id mappings\n",
    "model.config.id2label = label_mappings\n",
    "model.config.label2id = {v: k for k, v in label_mappings.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa874fa5-f7d5-449f-bdea-4d78b5637165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 1:17:46, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.619712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.631500</td>\n",
       "      <td>1.565370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.631500</td>\n",
       "      <td>1.515470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.545600</td>\n",
       "      <td>1.469467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.475200</td>\n",
       "      <td>1.428275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.475200</td>\n",
       "      <td>1.394433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.416200</td>\n",
       "      <td>1.368611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.416200</td>\n",
       "      <td>1.349045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.381500</td>\n",
       "      <td>1.337835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.364100</td>\n",
       "      <td>1.334437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3000, training_loss=1.4690140380859376, metrics={'train_runtime': 4668.4585, 'train_samples_per_second': 2.57, 'train_steps_per_second': 0.643, 'total_flos': 604349483472000.0, 'train_loss': 1.4690140380859376, 'epoch': 10.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "# Convert labels to numeric format\n",
    "train_labels = torch.tensor(train_df['Sentiment'].factorize()[0])\n",
    "val_labels = torch.tensor(val_df['Sentiment'].factorize()[0])\n",
    "\n",
    "# Create a Dataset class to work with Trainer\n",
    "class SentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SentimentDataset(train_encodings, train_labels)\n",
    "val_dataset = SentimentDataset(val_encodings, val_labels)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-7,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5bcfd2f4-e35f-4510-9c7a-c611a32a03db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 00:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.3344367742538452,\n",
       " 'eval_runtime': 23.7275,\n",
       " 'eval_samples_per_second': 12.644,\n",
       " 'eval_steps_per_second': 3.161,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model and tokenizer\n",
    "model.save_pretrained(\"./fine_tuned_sentiment_model_rt2\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_sentiment_model_rt2\")\n",
    "\n",
    "# Evaluate the model on the validation dataset\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980ba408-a9f2-48ff-8f3b-5ed0deff0801",
   "metadata": {},
   "source": [
    "### Micro, Macro Precision and Recall Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec241f51-5ccf-481a-9171-c0c7b973fe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ead6e0c9-a066-4774-9af8-13576da1ee75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get predictions from the model\n",
    "def get_predictions(model, dataset):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in tqdm(dataset):\n",
    "        inputs = {key: batch[key].unsqueeze(0).to(model.device) for key in batch if key != 'labels'}\n",
    "        labels = batch['labels'].to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        # Convert labels and predictions to list format\n",
    "        all_labels.append(labels.item())  # For single labels\n",
    "        all_preds.append(predictions.item())  # For single predictions\n",
    "    \n",
    "    return all_labels, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97917a59-3c59-40ed-8996-1a6f9909197b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 300/300 [00:29<00:00, 10.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': {'precision': 0.5909090909090909, 'recall': 0.65, 'f1-score': 0.6190476190476191, 'support': 60.0}, '1': {'precision': 0.43636363636363634, 'recall': 0.4, 'f1-score': 0.41739130434782606, 'support': 60.0}, '2': {'precision': 0.6, 'recall': 0.55, 'f1-score': 0.5739130434782609, 'support': 60.0}, '3': {'precision': 0.48, 'recall': 0.4, 'f1-score': 0.43636363636363634, 'support': 60.0}, '4': {'precision': 0.5135135135135135, 'recall': 0.6333333333333333, 'f1-score': 0.5671641791044776, 'support': 60.0}, 'accuracy': 0.5266666666666666, 'macro avg': {'precision': 0.5241572481572482, 'recall': 0.5266666666666666, 'f1-score': 0.5227759564683641, 'support': 300.0}, 'weighted avg': {'precision': 0.5241572481572482, 'recall': 0.5266666666666666, 'f1-score': 0.5227759564683641, 'support': 300.0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get predictions and labels from the validation set\n",
    "true_labels, predicted_labels = get_predictions(model, val_dataset)\n",
    "\n",
    "# Compute classification metrics: precision, recall, f1, and support\n",
    "report = classification_report(true_labels, predicted_labels, labels=[0,1,2,3,4], output_dict=True)\n",
    "\n",
    "# Print out the classification report\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6fd2883f-7350-429e-a2a3-e247ede07c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro Precision: 0.5241572481572482\n",
      "Macro Recall: 0.5266666666666666\n",
      "Micro Precision: 0.5241572481572482\n",
      "Micro Recall: 0.5266666666666666\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Extract precision, recall, and support for each class\n",
    "precision_per_class = [report[str(label)]['precision'] for label in range(len(report) - 3)]\n",
    "recall_per_class = [report[str(label)]['recall'] for label in range(len(report) - 3)]\n",
    "support_per_class = [report[str(label)]['support'] for label in range(len(report) - 3)]\n",
    "\n",
    "# Calculate macro-averaged metrics (which are directly available in `report`)\n",
    "macro_precision = report['macro avg']['precision']\n",
    "macro_recall = report['macro avg']['recall']\n",
    "\n",
    "# Calculate micro-averaged metrics\n",
    "# Weighted by the number of samples per class (support)\n",
    "micro_precision = np.sum([p * s for p, s in zip(precision_per_class, support_per_class)]) / np.sum(support_per_class)\n",
    "micro_recall = np.sum([r * s for r, s in zip(recall_per_class, support_per_class)]) / np.sum(support_per_class)\n",
    "\n",
    "# Output calculated values\n",
    "print(\"Macro Precision:\", macro_precision)\n",
    "print(\"Macro Recall:\", macro_recall)\n",
    "print(\"Micro Precision:\", micro_precision)\n",
    "print(\"Micro Recall:\", micro_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706370f5-3f50-4948-b5d8-e66888a6eece",
   "metadata": {},
   "source": [
    "## Test some Sample Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "204bfb89-65c7-462c-b8af-438453c745ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"fine_tuned_sentiment_model_rt2\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"fine_tuned_sentiment_model_rt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb534d1f-55af-4b2b-ba46-bfb58fdbd8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"text-classification\", model=\"fine_tuned_sentiment_model_rt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a885ce7e-bcc2-47e9-9497-767f66b52b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = [\"The food was absolutely horrible. The flavors were completely unbalanced and the presentation was terrible. I wouldn\\u2019t recommend this place to my worst enemy.\",\n",
    "             \"While this restaurant had some good dishes, some of my friends ordered the same thing, which resulted in an overcrowded table. The dining area was quite small, which made us feel rushed.\",\n",
    "            \"This place has been on my bucket list for so long! The food is always a highlight, and I love trying new dishes. I highly recommend it to anyone looking for a great dining experience.\",\n",
    "             \"The food was okay, nothing special. It was neither bad nor great, just an average meal.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1ec3fbe3-d103-4f5c-94ce-8550e8ba98ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: The food was absolutely horrible. The flavors were completely unbalanced and the presentation was terrible. I wouldn’t recommend this place to my worst enemy.\n",
      "Predicted Label: Strong Negative,Score: 0.4599\n",
      "\n",
      "Input: While this restaurant had some good dishes, some of my friends ordered the same thing, which resulted in an overcrowded table. The dining area was quite small, which made us feel rushed.\n",
      "Predicted Label: Strong Negative,Score: 0.3145\n",
      "\n",
      "Input: This place has been on my bucket list for so long! The food is always a highlight, and I love trying new dishes. I highly recommend it to anyone looking for a great dining experience.\n",
      "Predicted Label: Strong Positive,Score: 0.3114\n",
      "\n",
      "Input: The food was okay, nothing special. It was neither bad nor great, just an average meal.\n",
      "Predicted Label: Mild Negative,Score: 0.3300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text in test_texts:\n",
    "    result = classifier(text)\n",
    "    print(f\"Input: {text}\")\n",
    "    print(f\"Predicted Label: {result[0]['label']},Score: {result[0]['score']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4a1dd0-1f6a-4bbe-b534-557e1c6dbcde",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a92a7bb-a70e-4b33-b451-9bf951426c92",
   "metadata": {},
   "source": [
    "Clearly, running the model with more epochs has a better outcome. \n",
    "\n",
    "I would say, for this particular model:\n",
    "<ul>\n",
    "    <li>Reducing the batch size, because of how similar the samples were when we generated the samples certainly helps. I suspect, why reducing the batch size matters is because it's like working with one individual sample.</li>\n",
    "    <li>Increasing the number of epochs will certainly help better fit the model, if better resources are available, we could increase the number.</li>\n",
    "    <li>We could definitely tinker with the weight decay and learning rate as well. This model can definitely perform better than what it is now.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd579e70-32d5-4045-bd20-8b9acd208914",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
